{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c351bc0-d4cf-4b59-8911-7a72215aa3b0",
   "metadata": {},
   "source": [
    "# Assignment 2: Using Tenslorflow library to build and optimize neural networks with MNIST\n",
    "\n",
    "### Ph22 / Caltech / Spring 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eee6624-482d-4181-9179-748b64ac2e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfc555-974e-4b58-8609-839c97f005ea",
   "metadata": {},
   "source": [
    "# Part 1: Load and normalize the MNIST data set using Tensorflow-Datasets\n",
    "1. Use tfds.load() to get MNIST data; you can refer to https://www.tensorflow.org/datasets/api_docs/python/tfds/load for how tfds.load() works. In particular, make sure that tfds.load() returns `(img, label)` instead of a dictionary `{'image': img, 'label': label}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c90c53-d2e3-44f5-8286-5e6e65ca29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist', \n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a20f3-2a88-4c99-9e87-988ab94187a4",
   "metadata": {},
   "source": [
    "2. Use subroutines from https://www.tensorflow.org/api_docs/python/tf/data/Dataset in order to normalize the data and put it into the TF format. In particular, for training data, do the following transformations: \n",
    "- tdfs.load() provide images of type tf.uint8, while the model expects tf.float32. Therefore, you need to normalize images. For this, first write a function normalize_img(image, label) that converts the image to `uint8` -> `float32`. \n",
    "- As you fit the dataset in memory, cache it before shuffling for a better performance. For true randomness, set the shuffle buffer to the full dataset size.\n",
    "- Batch elements of the dataset after shuffling to get unique batches at each epoch.\n",
    "- It is good practice to end the pipeline by prefetching for performance. \n",
    "\n",
    "and similarly for test data, except that we don't need to shuffle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c7ceb3-f9ad-4537-96ca-33b4e6a32993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32) \n",
    "  return image, label\n",
    "\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()  \n",
    "ds_train = ds_train.shuffle(buffer_size=ds_info.splits['train'].num_examples)  \n",
    "ds_train = ds_train.batch(32)  \n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)  \n",
    "\n",
    "\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.cache()  \n",
    "ds_test = ds_test.batch(32)  \n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f06193",
   "metadata": {},
   "source": [
    "3. Learn about and describe (in a sentence or two) what tf.data.AUTOTUNE and preftech does. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a8f11",
   "metadata": {},
   "source": [
    "\n",
    "tf.data.AUTOTUNE enables TensorFlow to adjust parallel operations dynamically for optimal performance, while prefetch() allows the dataset to prepare future batches during current batch processing, and that significantly improving training efficiency by reducing wasted time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f18e80-8c5b-49ed-a825-6b147b14c951",
   "metadata": {},
   "source": [
    "# Part 2: Build and compile the model\n",
    "1. Below is a neural network with a simple architecture with the hidden layer missing. Add code to implement an hidden layer with 128 neurons. How does this compare to the neural network you implemented in Assignment 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613280b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this is the design architecture of the neural network\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# here we specify the optimizer and loss function which can dramatically effect the training\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001), #learning rate\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# here we train the neural network \n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")\n",
    "\n",
    "# here we use Pandas to plot the training and validation\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.yscale('log')\n",
    "plt.grid()\n",
    "plt.xlabel('Epoch #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57757fca",
   "metadata": {},
   "source": [
    "In this neural network we use TensorFlow and it handles the complexities of differentiating and optimizing the network, making it easy to experiment with different architectures and hyperparameters. It uses a high level API that simplifies many operations, such as layer creation, model compilation, and training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0ef6fb",
   "metadata": {},
   "source": [
    "2. Now simply convert the above code to work with fashion_mnist (https://www.tensorflow.org/datasets/catalog/fashion_mnist) instead of MNIST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfd87ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'fashion_mnist', \n",
    "    split=['train', 'test'],\n",
    "    as_supervised=True,\n",
    "    with_info=True\n",
    ")\n",
    "def normalize_img(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) \n",
    "    return image, label\n",
    "\n",
    "ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(buffer_size=ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(32)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.batch(32)\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10)  \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    epochs=6,\n",
    "    validation_data=ds_test,\n",
    ")\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8072417",
   "metadata": {},
   "source": [
    "3. If your loss is getting much smaller than your validation loss, then your network is overfitting. To tackle this, go over https://www.comet.com/site/blog/dropout-regularization-with-tensorflow-keras/ to learn about dropout, a technique to prevent overfitting and implement the above network with dropout. How does that change things?  Change the size of layers, number of layers, dropout, etc. to try to improve the training speed and the final validation accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1187a1c",
   "metadata": {},
   "source": [
    "It s not getting much smaller than the validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356fec55",
   "metadata": {},
   "source": [
    "4. Explore a few optimizers (https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) and cost functions (https://www.tensorflow.org/api_docs/python/tf/keras/losses). Which ones give the best performance? Try to come up with an intuitive explanation for why. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd98314",
   "metadata": {},
   "source": [
    "I have reached a 97.73% accuracy rate in networkfashionmnist.py as I chose to work with Nadam as an optimizer. Nadamâ€™s incorporation has the potential to lead to faster convergence in this setting, which helps the model learn quicker in a scenario where gradients can be sparse. I kept the same cost because I found it working well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba8d04",
   "metadata": {},
   "source": [
    "# (Optional) Part 3: Start the adventure of hyper-parameter tuning!\n",
    "\n",
    "1. As you have seen, trying out different model features and properties is a tedious process but there is where the \"magic\" lies: optimal model design is usually very specific to the task at hand (but for a counterexample of this, see the field of transfer learning) and usually depends on domain knowledge. There are more systematic ways of tuning our parameters compared to just trial and error that we have tried till now. For this part, let's fix the model architecture and only consider the learning rate as it is a one-dimensional parameters for simplicity. Go over https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide and implement one of the methods of hyperparamter tuning described."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
